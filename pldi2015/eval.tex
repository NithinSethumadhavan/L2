\begin{figure*}[c]\centering
  \small
  \begin{tabular}{| l | l | c | c | c | c | c | c | c | p{6.3cm} |}
    \hline
    & Name 
    & \rotatebox[origin=c]{90}{Runtime}
    & \rotatebox[origin=c]{90}{~~Runtime (no deduction)~~}
    & \rotatebox[origin=c]{90}{Runtime (no types)}
    & \rotatebox[origin=c]{90}{Expert examples} 
    & \rotatebox[origin=c]{90}{~~Random examples~~} 
    & \rotatebox[origin=c]{90}{Runtime (random)}
    & \rotatebox[origin=c]{90}{Extra primitives} 
    & Description \\
    \hline

\multirow{18}{*}{\rotatebox[origin=c]{90}{Lists}}
& add & \textbf{0.04} & 0.05 & 3.87 & 5 & 4 & \textbf{0.04} &  & Add a number to each element of a list. \\
& append & \textbf{0.23} & 0.49 & $\bot$ & 3 & 16 & 0.93 &  & Append an element to a list. \\
& concat & \textbf{0.13} & 0.22 & 68.95 & 5 & 23 & 0.20 &  & Concatenate two lists together. \\
& dedup & \textbf{231.05} & $\bot$ & $\bot$ & 7 & - & - & member & Remove duplicate elements from a list. \\
& droplast & \textbf{316.39} & $\bot$ & $\bot$ & 6 & - & - &  & Drop the last element in a list. \\
& dropmax & \textbf{0.12} & 0.19 & 77.05 & 3 & 7 & 0.16 & max & Drop the largest number(s) in a list. \\
& dupli & \textbf{0.11} & 0.86 & 378.35 & 3 & 5 & 0.20 &  & Duplicate each element of a list. \\
& evens & \textbf{7.39} & 45.52 & $\bot$ & 5 & 8 & 30.08 &  & Remove the odd numbers from a list. \\
& last & \textbf{0.02} & 0.06 & 1.80 & 4 & 4 & 0.03 &  & Return the last element in a list. \\
& length & \textbf{0.01} & 0.14 & 41.36 & 4 & 5 & 0.04 &  & Return the length of a list. \\
& max & \textbf{0.46} & 9.53 & $\bot$ & 7 & 8 & 8.19 &  & Return the largest number in a list. \\
& member & \textbf{0.35} & 2.87 & $\bot$ & 8 & 88 & 1.15 &  & Check whether an item is a member of a list. \\
& multfirst & \textbf{0.01} & \textbf{0.01} & 1.82 & 4 & 5 & 0.03 &  & Replace every item in a list with the first item. \\
& multlast & \textbf{0.08} & 0.51 & $\bot$ & 4 & 7 & 0.27 &  & Replace every item in a list with the last item. \\
& reverse & \textbf{0.01} & 0.06 & 39.03 & 4 & 5 & 0.03 &  & Reverse a list. \\
& shiftl & \textbf{0.89} & 6.23 & $\bot$ & 5 & 7 & 2.19 & reverse & Shift all elements in a list to the left. \\
& shiftr & \textbf{0.65} & 3.79 & $\bot$ & 6 & 13 & 6.58 & reverse & Shift all elements in a list to the right. \\
& sum & \textbf{0.01} & 0.31 & 44.24 & 4 & 4 & 0.04 &  & Return the sum of a list of integers. \\
\hline
\multirow{11}{*}{\rotatebox[origin=c]{90}{Trees}}
& count\_leaves & \textbf{0.44} & 2.69 & $\bot$ & 8 & 10 & 0.67 & sum & Count the number of leaves in a tree. \\
& count\_nodes & \textbf{0.62} & 6.13 & $\bot$ & 4 & 9 & 1.04 &  & Count the number of nodes in a tree. \\
& flatten & \textbf{0.08} & 0.09 & 102.24 & 3 & 6 & 0.14 & join & Flatten a tree into a list. \\
& height & \textbf{0.10} & 0.27 & 83.12 & 6 & 7 & 0.20 & max & Return the height of a tree. \\
& incrt & 0.02 & \textbf{0.01} & 1.90 & 3 & 4 & 0.03 &  & Increment each node in a tree by one. \\
& leaves & \textbf{0.52} & 1.83 & $\bot$ & 5 & 8 & 0.83 & join & Return a list of the leaves of a tree. \\
& maxt & \textbf{10.59} & 375.07 & $\bot$ & 6 & 43 & 46.80 &  & Return the largest number in a tree. \\
& membert & \textbf{4.66} & 56.80 & $\bot$ & 12 & 75 & 18.07 &  & Check whether an element is contained in a tree. \\
& selectnodes & \textbf{15.97} & 94.91 & $\bot$ & 4 & 9 & 66.81 &
join, {\tt pr} & Return a list of nodes in a tree that match a
predicate {\tt pr}. \\
& sumt & \textbf{0.59} & 5.74 & $\bot$ & 3 & 9 & 1.06 &  & Sum the nodes of a tree of integers. \\
& tconcat & \textbf{551.84} & $\bot$ & $\bot$ & 3 & - & - &  & Insert a tree under each leaf of another tree. \\
\hline
\multirow{12}{*}{\rotatebox[origin=c]{90}{Nested structures}}
& appendt & \textbf{1.03} & 2.44 & $\bot$ & 5 & 14 & 2.57 &  & Append an element to each node in a tree of lists. \\
& cprod & \textbf{83.83} & $\bot$ & $\bot$ & 4 & - & - &  & Return the cartesian product of a list if lists. \\
& dropmins & \textbf{114.65} & 452.07 & $\bot$ & 4 & - & - & min & Drop the smallest number in a list of lists. \\
& flattenl & \textbf{0.08} & 0.11 & 87.94 & 5 & 5 & 0.15 & join & Flatten a tree of lists into a list. \\
& incrs & \textbf{0.12} & 0.80 & $\bot$ & 4 & 4 & 0.46 &  & Increment each number in a list of lists. \\
& join & \textbf{0.43} & 2.13 & $\bot$ & 4 & 6 & 1.01 &  & Concatenate a list of lists together. \\
& prependt & \textbf{0.01} & \textbf{0.01} & 3.46 & 5 & 4 & 0.03 &  & Prepend an element to each list in a tree of lists. \\
& replacet & \textbf{4.02} & 10.22 & $\bot$ & 4 & 8 & 10.94 &  & Replace one element with another in a tree of lists. \\
& searchnodes & \textbf{4.28} & 43.85 & $\bot$ & 6 & 31 & 19.68 & member & Check if an element is contained in a tree of lists. \\
& sumnodes & \textbf{0.16} & 0.43 & $\bot$ & 4 & 3 & 0.34 &  & Replace each node with its sum in a tree of lists. \\
& sums & \textbf{0.12} & 1.26 & $\bot$ & 4 & 5 & 0.54 &  & For each list in a list of lists, sum the list. \\
& sumtrees & \textbf{12.10} & 77.21 & $\bot$ & 3 & 5 & 49.55 &  &
Return the sum of each tree in a list of trees. \\
\hline


\multicolumn{2}{|c|}{Median} & 0.43 & 2.13 & $\bot$ & 4 & 8 & 0.93 & \multicolumn{2}{|c|}{} \\
\hline

  \end{tabular}
  \caption{\sys performance. Times are in
    seconds. $\bot$ indicates a timeout ($>$10 minutes) or an out of
    memory condition ($>$8GB).}
\label{fig:example-desc}
\end{figure*}


We have implemented the proposed synthesis algorithm  in
a tool called \sys, which consists of
$\sim$4,000 lines of OCaml code. In our current implementation, the
cost function prioritizes open hypothesis generation over brute-force
search for closed hypotheses. In particular, this is done by using a
weighting function $W(c_a, c_b) = c_a + 1.5 ^{c_b}$ where $c_a$ and
$c_b$ are the total costs of expressions obtained through open and
closed hypothesis generation respectively.  Intuitively, the weighting
function attempts to balance the relative value of continued
generalization and exhaustive search --- the exponential term reflects
that the exhaustive search space grows exponentially with maximum cost.


To evaluate our synthesis algorithm, we gathered a corpus of over
40  data structure manipulation tasks involving lists, trees, and nested data structures 
such as lists of lists or trees of lists.  As described in the last column of
Figure~\ref{fig:example-desc},
%Our synthesis tasks include list and tree-manipulation
%examples, as well as various tasks that involve nested data
%structures, such as lists of lists or trees of lists.  Most of the
most of our benchmarks are textbook examples for
functional programming and some are inspired by end-user
synthesis tasks, such as those mentioned in~\secref{intro}
and~\secref{example}.

Our main goal in the experimental evaluation is to assess (i) whether
\sys is able to synthesize the examples we collected, (ii) how long
each synthesis task takes, and (iii) how many examples need to be
provided by the user for \sys to generate the intended program.  To
answer these questions, we conducted an experimental evaluation by
running \sys over our benchmark examples on an Intel(R) Xeon(R)
E5-2430 CPU (2.20 GHz) with 8GB of RAM.

The column labeled ``Runtime" in Figure~\ref{fig:example-desc} shows
the running time of \sys on each benchmark. The symbol $\bot$
indicates that \sys is unable to complete the synthesis task within a
time limit of 10 minutes. As shown in Figure~\ref{fig:example-desc},
\sys is able to successfully synthesize every benchmark program within
its resource limits. Furthermore, we see that \sys is quite efficient:
its median runtime is 0.43 seconds, and it can synthesize
%68\% of the benchmarks in under a second, and 
88\% of the benchmarks in under a minute.
% and its median runtime is 0.43 seconds.
%Finally, every example takes under 10 minutes to synthesize.

The column labeled ``Expert examples" shows the number of examples we
provided for each synthesis task.  As shown in
Figure~\ref{fig:example-desc}, more than $75\%$ of the benchmarks
require 5 or fewer input-output examples, and there is only a single
benchmark that requires more than 10 examples.
%, and the median number of examples is only 4. 

Additionally, we are also interested in investigating the following questions:
\begin{itemize}
\item What is the impact of using types in our algorithm?
\item How effective is deduction?
\item How does \sys behave if its examples are not
  provided by an expert who is familiar with \sys?
\end{itemize}
In what follows, we address these questions in more detail.

\paragraph{Impact of types.} Recall that our synthesis algorithm uses
type-aware inductive generalization to prune the search space. To
understand the impact of using types, we modified our algorithm to
ignore types when generating hypotheses. The column labeled ``Runtime
(no types)" in Figure~\ref{fig:example-desc} shows the running time of
the algorithm when we do \emph{not} perform inductive generalization
in a type-aware way. As shown by the data, types have a huge impact on
the running time of the synthesis algorithm. In fact, without
type-aware inductive generalization, more than $60\%$ of the
benchmarks do not terminate within the provided 10 minute resource
limit.


\paragraph{Impact of deduction.} We also conducted an experiment to
evaluate the effectiveness of deduction in the overall synthesis
algorithm. Recall from \secref{algo} that our algorithm uses deduction
for (i) inferring new input-output examples, and (ii) refuting 
incorrect hypotheses. To evaluate the impact of deduction, we modified
our algorithm so that it does not perform any of the reasoning
described in \secref{deduction}. The running times of this modified
algorithm are presented in Figure~\ref{fig:example-desc} under the
column labeled ``Runtime (no deduction)". While the impact of
deduction is not as dramatic as the impact of type-aware hypothesis
generation, it nonetheless has a significant impact. In
particular, the original algorithm with deduction is, on average, 6
times faster than the modified algorithm with no deduction.


\paragraph{Random example generation.} Next, we examined the extent to
which the effectiveness of our algorithm depends on the quality of
user-provided examples. In particular, we aimed to estimate the
behavior of \sys on examples provided by a user who has no prior
exposure to program synthesis tools.
%since the
%co-author who supplied the examples knows \sys quite well, we also wanted to
%understand how \sys might behave
%runtime of \sys, and the number of examples it needs, 
%when the examples are provided by a lay user.  
To this end, we built a {\em random example generator} that serves as
a ``lower bound'' on a human user of \sys.  The random example
generator is, by design, quite {naive}.  For instance, given a program
with input type ${\tt list[int]}$, our example generator chooses a
small list length and %uniformly from a small interval,
then populates the list with integers generated uniformly from a small
interval.  The corresponding output is generated by running a known
implementation of the benchmark on this input. 
% An example set consisted of a number $k$ of such I/O examples,
% generated independently.

Now, to determine the minimum number of examples that \sys needs to
synthesize the benchmark program, we run the random example generator
to generate $k$ independent input-output examples. Given an example
set of size $k$, we then check whether \sys is able to synthesize the
correct function in 90\% of the trials.  If so, we conclude that the
lay user should be able to successfully synthesize the target program
with an example set of size $k$, and set the {\em runtime} of the
benchmark to be the median runtime on these trials. Otherwise, we
increase the value of $k$ and repeat this process.


\begin{comment}
Not all example sets generated as above would lead to the known
implementation, as there may be a simpler program that fits the
examples. Hence, for each value of $k$, we repeatedly ran the example
generator and \sys. For each $k$, if \sys did not
synthesize the correct function in 90\% of the trials, then we
concluded that the lay user would not be able to successfully
synthesize the target program with an example set of size $k$. In this
case we increased the value of $k$ and repeated the
process. Otherwise, we considered $k$ to be the {\em minimum number of
  random examples} that \sys needed for the benchmark, and calculated
the {\em median random runtime} of \sys for synthesis in trials
involving examples of size $k$.
\end{comment}

% In problems over trees, the depth was selected uniformly from
% $[0,3]$; the number of children for each node was generated
% uniformly from $[1,3]$.
%Surely, such a process is a lower bound on humans using our system.

%Since we can reasonably expect the average user to perform no worse
%than a random example generator, this experiment gives us further
%confidence about the validity of our results.

The columns labeled ``Random examples" and ``Runtime (random)'' in
\figref{example-desc} respectively show the minimum number of examples
and
%the %median  
runtime for each benchmark when the examples are generated randomly.
We see that the median values of these quantities are fairly
low (8 and 0.93 seconds, respectively).
%We see that, on average, \sys needs 8 randomly generated examples and
%completes the synthesis task in 0.93 seconds.
%\footnote{Here, we use
%  term ``average" to indicate the median.}.


For a few benchmarks (e.g., {\tt dedup}), we observe that \sys is
unable to synthesize the program for all trials with $k \le 100$
examples while it requires a large number of examples for a few other
benchmarks (e.g., {\tt member}). However, upon further inspection,
this turns out to be due to the naivet\'e of our random example
generator rather than a shortcoming of \sys. For instance, consider
the {\tt member} benchmark which returns true iff the input list $l$
contains a given element $e$. Clearly, any reasonable training set
should contain a mix of positive and negative examples.  However, when
both the list $l$ and the element $e$ are randomly generated, it is
very unlikely that $l$ contains $e$. Hence, many randomly generated
example sets contain only negative examples and fail to illustrate the
desired concept. However, we believe it is entirely reasonable to
expect that a human can provide both positive and negative examples in
the training set.

% , thereby allowing \sys to learn the desired concept.


%Finally, we were also interested in comparing the running time of the
%synthesis algorithm when the examples are provided by an expert user
%vs. generated randomly. Somewhat surprisingly, the running time of the
%algorithm in these two cases was identical in most cases.; hence, we do not
%include two separate runtimes in Figure~\ref{fig:example-desc}.

\paragraph{Summary.} Overall, our experimental evaluation validates
the claim that \sys can effectively synthesize representative,
non-trivial examples of data structure transformations.  Our
experiments also show that type-aware inductive generalization and
deductive reasoning have a significant impact on the running time of
the algorithm. Finally, our experiments with randomly generated
input-output examples suggest that using \sys requires no special skill
on the part of the user.




\begin{comment}
When we evaluated the performance of our system, we wanted to answer
the following questions:

\begin{enumerate}
\item How complex are \sys specifications? Are they easy for users to write?
\item Does our use of deduction improve runtime?
\item Is type-guided exhaustive search an improvement over type-naive search?
\end{enumerate}

To answer these questions, we built a corpus of problems that are
representative of real world programs. Many of these examples will be
familiar to functional programmers. They include various data
structure manipulation tasks on lists, trees, and nested
structures. See Figure~\ref{fig:example-desc} for the names and
descriptions of the problems in our corpus.

\begin{paragraph}{Specification complexity}
  We examined the required number of examples as a proxy for the
  complexity of a specification. The number of examples required for a
  correct synthesis should be as small as possible, to reduce the
  amount of work that the user must perform.

  To determine how many examples are required for a correct synthesis,
  we simulated user input by generating random examples and attempting
  synthesis. To generate a random example for a given problem, we
  generate a reasonably sized random input and run a known-good
  implementation on it to obtain the correct output. To determine the
  minimum required number of examples, we ran repeated trials with
  increasing numbers of examples until 90\% of the solutions were
  correct. The ``Random examples'' column of
  Figure~\ref{fig:example-desc} contains the minimum number of random
  examples required for at least 90\% correctness over 100 runs of
  \sys.

  Most of the problems require small numbers of examples, with a few
  exceptions. Random example generation works poorly when the problem
  involves searching for a value or checking for membership. This is
  because when we generate, for example, a random list and an element
  to search for, it is likely that the element will not be present in
  the list. For search or membership problems, most of the examples
  will be of failure of the search or membership check, so with a low
  number of examples, the problem will be underspecified. However, the
  number of expert examples needed for these functions is small, so
  the issue is primarily with the random example generation process
  rather than with \sys.

  To provide a comparison to the randomly generated examples, we also
  show the number of examples we used in our own specifications. We
  feel that our example sets are representative of input that an
  expert user might provide while synthesizing a program with \sys. In
  general, the number of examples are similar to those obtained by the
  random generation process. Our handwritten example sets are not
  optimized for size, so there are some cases where random example
  generation does better.

  The fact that randomly generated examples are competitive with our
  handwritten examples shows that writing specifications for \sys
  requires no special skill on the part of the user. Examples do not
  need to be carefully selected or tuned to the corner cases of a
  particular problem.
\end{paragraph}

\begin{paragraph}{Impact of deduction}
  The timing results show that the use of deduction has a significant
  positive impact, especially on larger problems. In particular, the
  greatest reduction in runtime is achieved for problems that perform
  large amounts of exhaustive search. This is because deduction prunes
  away abstract and concrete hypotheses, greatly reducing the number
  of incorrect or duplicate expressions that are searched. It is only
  for problems that run in hundredths of a second and do very little
  searching that the slight overhead of performing deduction may
  outweigh its benefits in reducing the search space. From these
  results we conclude that the use of deduction is purely beneficial.
\end{paragraph}

\begin{paragraph}{Untyped search}
  As we can see in Figure~\ref{fig:example-desc}, if we replace our
  exhaustive search with a variant that ignores types, \sys times out
  on over half of the problems and is slow to solve the remainder. The
  problems that do complete in a relatively short amount of time are
  those that require very little exhaustive search. This illustrates
  the significant amount of pruning that we get from requiring the
  exhaustive search to generate correctly typed expressions. As with
  deduction, the additional complexity of pruning the exhaustive
  search using types is worthwhile because of the increase in
  performance.
\end{paragraph}

\begin{paragraph}{Cost model}
  The last critical factor in our implementation's performance is the
  cost model. As described in Section~\ref{sec:problem}, \sys searches
  programs in order of least cost. Specifically, when we exhaustively
  search for expressions to fill the holes of an abstract hypothesis,
  we calculate the cost of the abstract hypothesis and the
  expressions, and then combine the costs with a weighting function
  $W(c_a, c_e)$ to determine the cost of the entire program. In our
  current implementation, $W(c_a, c_e) = c_a + 1.5^{c_e}$. The
  weighting function attempts to balance the relative value of
  continued generalization and exhaustive search. The exponential term
  reflects the fact that the exhaustive search space grows
  exponentially as the maximum cost increases. We find that in
  practice this weighting function produces good results on our
  corpus.

  Each primitive operator and combinator has an assigned cost, which
  we use to calculate the cost of a program inductively. In our
  implementation, primitive operators all have unit cost. We
  experimented with various cost models and found that weighting
  against particular operators was a performance negative. Combinators
  are assigned a higher cost than primitives, which helps balance
  inductive generalization with search.
\end{paragraph}

\begin{paragraph} {Timing}
  The running times in Figure~\ref{fig:example-desc} use our
  hand-written example sets. We examined runtimes from the random
  example sets and found that there is no significant difference, so
  these runtimes are not provided. The ``Runtime (no deduction)''
  column contains runtimes with deduction turned off for both
  inductive generalization and exhaustive search. The ``Runtime (no
  types)'' column contains runtimes with deduction turned off and
  using a modified exhaustive search that ignores types. Timing
  experiments were run on an Intel(R) Xeon(R) E5-2430 CPU (2.20GHz)
  with 8 Gb of RAM.
\end{paragraph}

\begin{paragraph}{Implementation} Our implementation is
  single-threaded and is written in approximately 4000 lines of
  OCaml. It includes the synthesis algorithm as well as a parser and
  interpreter for the language described in Section~\ref{sec:problem}.
\end{paragraph}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "pldi"
%%% End: 


\end{comment}
